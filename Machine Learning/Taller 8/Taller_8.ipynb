{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Taller_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SenAd9tDRtL6"
      },
      "source": [
        "![image](https://drive.google.com/u/0/uc?id=15DUc09hFGqR8qcpYiN1OajRNaASmiL6d&export=download)\n",
        "\n",
        "# **Taller No. 8 - ISIS4825**\n",
        "## **Proceso de Aprendizaje Automático e Introducción a la Clasificación**\n",
        "## **Contenido**\n",
        "1. [**Objetivos**](#id1)\n",
        "2. [**Problema**](#id2)\n",
        "3. [**Importando las librerías necesarias para el laboratorio**](#id3)\n",
        "4. [**Visualización y Análisis Exploratorio**](#id4)\n",
        "5. [**Preparación de los Datos**](#id5)\n",
        "6. [**Modelamiento**](#id6)\n",
        "7. [**Predicción**](#id7)\n",
        "8. [**Validación**](#id8)\n",
        "9. [**Trabajo Asíncrono**](#id9)\n",
        "\n",
        "## **Objetivos**<a name=\"id1\"></a>\n",
        "- Familiarizarse con las librerías de Scikit-Learn y con el algoritmo de KNN\n",
        "- Resolver un problema de clasificación multiclase y tomar métricas de desempeño sobre este\n",
        "\n",
        "## **Problema**<a name=\"id2\"></a>\n",
        "- En una tienda de ropa buscan crear un algoritmo de clasificación que asigne una etiqueta a 10 tipos de prendas distintas. Desde ropa hasta accesorios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8yVM9-WGBUX"
      },
      "source": [
        "## **Notebook Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynONleF2GDXw"
      },
      "source": [
        "!shred -u setup_colab.py\n",
        "!wget -q \"https://github.com/jpcano1/ISIS_4825_Imagenes_Vision/raw/main/Machine%20Learning/setup_colab.py\" -O setup_colab.py\n",
        "import setup_colab as setup\n",
        "setup.setup_workshop_8()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2-dS9koRtL7"
      },
      "source": [
        "## **Importando las librerías necesarias para el laboratorio**<a name=\"id3\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnPKtL7b8Wz"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import (train_test_split, ShuffleSplit, \n",
        "                                     cross_val_score, GridSearchCV)\n",
        "from sklearn.metrics import (precision_score, recall_score, confusion_matrix, \n",
        "                             accuracy_score, f1_score, roc_curve, \n",
        "                             precision_recall_curve)\n",
        "\n",
        "import utils.general as gen\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-1Nr9qfaynJ"
      },
      "source": [
        "## **Visualización y Análisis Exploratorio**<a name=\"id4\"></a>\n",
        "- Vamos a hacer uso del Dataset `Fashion-MNIST` que consta de 10 clases:\n",
        "    0. T-Shirt/Top\n",
        "    1. Trouser\n",
        "    2. Pullover\n",
        "    3. Dress\n",
        "    4. Coat\n",
        "    5. Sandal\n",
        "    6. Shirt \n",
        "    7. Sneaker\n",
        "    8. Bag\n",
        "    9. Ankle Boot\n",
        "- De igual forma, el dataset tiene 70.000 imágenes en escala de grises con resolución 28x28. Sin embargo, las imágenes ya se encuentran aplanadas con tamaño de vector 784 componentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGhhg8x0cNFq"
      },
      "source": [
        "fashion_mnist = datasets.fetch_openml(\"Fashion-MNIST\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlzWWnskcrq_"
      },
      "source": [
        "data, target = fashion_mnist.data, fashion_mnist.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auxFWqN-dGus"
      },
      "source": [
        "data.shape, target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1agKeVhm5CM0"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(data)), 9)\n",
        "gen.visualize_subplot(\n",
        "    data[random_sample].reshape(-1, 28, 28),\n",
        "    target[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1T854QK5w77"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(data)), 9)\n",
        "gen.visualize_subplot(\n",
        "    data[random_sample].reshape(-1, 28, 28),\n",
        "    target[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh4PT97c7wtr"
      },
      "source": [
        "target_classes = [\"T-Shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \n",
        "                  \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8tlKKIE7HrE"
      },
      "source": [
        "target_distribution = pd.Series(target).value_counts().sort_index()\n",
        "target_distribution.index = target_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34fORFSH8OMd"
      },
      "source": [
        "target_distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIdc1cZ3a09O"
      },
      "source": [
        "## **Preparación de los Datos**<a name=\"id5\"></a>\n",
        "- Dado que es un algoritmo que requiere vectores, vamos a necesitar que todas nuestras imágenes sean aplanadas, si es que aún no lo están.\n",
        "\n",
        "### **Tratamiento de Imágenes**\n",
        "- En este caso, nuestras imágenes ya son vectores, sin embargo vamos a ver cómo hacer su transformación vector-imagen e imagen-vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E_f9YVR9MnQ"
      },
      "source": [
        "sample_img = data[0].reshape(28, 28)\n",
        "sample_target = target[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU-RdiCs9QO4"
      },
      "source": [
        "gen.imshow(sample_img, color=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFH2Gopm-JYl"
      },
      "source": [
        "sample_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOguFqLu_eZP"
      },
      "source": [
        "sample_img = sample_img.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq2aCs-v_rPL"
      },
      "source": [
        "sample_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4FC_RESjFeS"
      },
      "source": [
        "sample_img.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRsMYU4-tsZN"
      },
      "source": [
        "sample_img.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBNcJ5d7_3oR"
      },
      "source": [
        "### **Train Set, Validation Set, Test Set**\n",
        "- Generalmente, en el mundo del computer vision, se hace la siguiente partición de datasets:\n",
        "    - Train Data:\n",
        "        - Train Set\n",
        "        - Validation Set\n",
        "    - Test Data:\n",
        "        - Test Set\n",
        "- La partición de los datasets la podemos hacer de varias formas, pero en esta ocasión veremos la partición por índices y por contenido.\n",
        "\n",
        "#### **Partición por Índice**\n",
        "- Buscamos dividir nuestro dataset a partir de sus índices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjd3IfxWFQr8"
      },
      "source": [
        "rnd_data = np.random.choice(np.arange(len(data)), 10000)\n",
        "full_data = data.copy()\n",
        "full_target = target.copy()\n",
        "data = data[rnd_data]\n",
        "target = target[rnd_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gomDeVj2jnK"
      },
      "source": [
        "- Aquí usamos el `random_state` para definir una semilla de aleatoriedad para que los grupos generados se mantengan siempre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iE3hK2wAUdl"
      },
      "source": [
        "ss_full_train_test = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7gowYCxDHTv"
      },
      "source": [
        "for full_train_index, test_index in ss_full_train_test.split(data):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGEKr8myDMdD"
      },
      "source": [
        "full_train_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S0cHvacDNw5"
      },
      "source": [
        "test_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLu1jc0jEGM0"
      },
      "source": [
        "full_train_set, test_set = ((data[full_train_index], target[full_train_index]), \n",
        "                            (data[test_index], target[test_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX4yKr2jD6u2"
      },
      "source": [
        "ss_train_val = ShuffleSplit(n_splits=10, test_size=0.2, random_state=5678)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QYUrn5GEBCY"
      },
      "source": [
        "for train_index, val_index in ss_train_val.split(full_train_set[0]):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BmNCiLFFQHD"
      },
      "source": [
        "train_set, val_set = ((full_train_set[0][train_index], full_train_set[1][train_index]), \n",
        "                      (full_train_set[0][val_index], full_train_set[1][val_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvyDHWq3Fghq"
      },
      "source": [
        "X_train, y_train = train_set[0], train_set[1]\n",
        "X_val, y_val = val_set[0], val_set[1]\n",
        "X_test, y_test = test_set[0], test_set[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDrwl4TDFtZy"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjG6ShXNFxVH"
      },
      "source": [
        "X_val.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0btHvcfF0Pd"
      },
      "source": [
        "X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j5YUi0i2_PM"
      },
      "source": [
        "- Generemos una muestra de imágenes por cada set generado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUt-yQ9NGW0H"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_train)), 9)\n",
        "gen.visualize_subplot(\n",
        "    X_train[random_sample].reshape(-1, 28, 28),\n",
        "    y_train[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY1NnYsWGyIx"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_val)), 9)\n",
        "gen.visualize_subplot(\n",
        "    X_val[random_sample].reshape(-1, 28, 28),\n",
        "    y_val[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xngq2dD0G8ig"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_test)), 9)\n",
        "gen.visualize_subplot(\n",
        "    X_test[random_sample].reshape(-1, 28, 28),\n",
        "    y_test[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bMIfi0bJXoW"
      },
      "source": [
        "#### **Partición por Contenido**\n",
        "- Aquí no buscamos partir nuestro dataset a partir de los índices que contiene, sino por el cuerpo de la data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgXR6nlkJUtR"
      },
      "source": [
        "full_X_train, X_test, full_y_train, y_test = train_test_split(data, target, \n",
        "                                                              test_size=0.2, \n",
        "                                                              random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYB248E0KVG1"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(full_X_train, full_y_train, \n",
        "                                                  test_size=0.2, \n",
        "                                                  random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIaJJt81KqLn"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_train)), 9)\n",
        "gen.visualize_subplot(\n",
        "    X_train[random_sample].reshape(-1, 28, 28),\n",
        "    y_train[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFCKzZTvK32t"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_val)), 9)\n",
        "gen.visualize_subplot(\n",
        "    X_val[random_sample].reshape(-1, 28, 28),\n",
        "    y_val[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPTbKOtmK8OX"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_test)), 9)\n",
        "gen.visualize_subplot(\n",
        "    X_test[random_sample].reshape(-1, 28, 28),\n",
        "    y_test[random_sample],  (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4UjOaAYbFPY"
      },
      "source": [
        "## **Modelamiento**<a name=\"id6\"></a>\n",
        "- A la hora de modelar los datos, buscamos un algoritmo que generalice la forma como los datos se comportan y con base en ello, pueda generar predicciones.\n",
        "\n",
        "### **K-Nearest-Neighbors**\n",
        "- En este caso, vamos a utilizar un algoritmo de modelado no lineal basado en vecindades o *neighborhoods*. Se trata de *K-Nearest Neighbors*.\n",
        "\n",
        "![image](https://miro.medium.com/max/3544/1*4F-q86XFr2-EsaAcz0Zu5A.png)\n",
        "\n",
        "> Tomado de [Towards Data Science](https://towardsdatascience.com/k-nearest-neighbor-python-2fccc47d2a55)\n",
        "\n",
        "- Este espacio lo tomamos, generalmente, para buscar algoritmos que puedan ser usados para modelar, y una vez encontrados, exploramos los hiperparámetros que podamos usar para mejorar los resultados de nuestras predicciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwURGKS25uDr"
      },
      "source": [
        "KNeighborsClassifier?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPUBsYJwkPNO"
      },
      "source": [
        "- En este caso, se utiliza el valor de vecinos por defecto, que es el `k=5`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqNktuMbNAcJ"
      },
      "source": [
        "knn_clf = KNeighborsClassifier(n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9z6B97fNOvZ"
      },
      "source": [
        "knn_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02kqqiUEbI8n"
      },
      "source": [
        "## **Predicción**<a name=\"id7\"></a>\n",
        "- En esta etapa nos concentramos en hacer nuestras predicciones y validarlas con el ojo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUFqcSMO6en"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_test)), 9)\n",
        "y_pred = knn_clf.predict(X_test[random_sample])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn8sNJ1d89Ah"
      },
      "source": [
        "gen.visualize_subplot(\n",
        "    X_test[random_sample].reshape(-1, 28, 28),\n",
        "    y_pred, (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar7LQHP0bLWp"
      },
      "source": [
        "## **Validación**<a name=\"id8\"></a>\n",
        "- En esta etapa de evaluación realizamos el proceso de toma de métricas. Por lo tanto, dado que estamos resolviendo un problema de clasificación, vamos a usar la matriz de confusión y las siguientes métricas:\n",
        "    - Accuracy score: $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "    - Precision: $\\frac{TP}{TP + FP}$\n",
        "    - Cobertura: $\\frac{TP}{TP + FN}$ (Recall, Sensitivity)\n",
        "    - F1 score: $\\frac{TP}{TP + \\frac{FN + FP}{2}}$ (Harmonic Mean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN5lWCNT9sFl"
      },
      "source": [
        "y_pred = knn_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWp7yoN6RtL8"
      },
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTV3vA-D9YqD"
      },
      "source": [
        "pd.DataFrame(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVavLlyZVT3_"
      },
      "source": [
        "- En esta matriz de confusión podemos ver claramente que las diagonales sobresalen, es decir, hubo un gran número de predicciones correctas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rU9QmOIkdb7"
      },
      "source": [
        "plt.matshow(conf_matrix, cmap=\"gray\")\n",
        "plt.grid(0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "femTRpn3mXaH"
      },
      "source": [
        "np.trace(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_SVf13XmfvZ"
      },
      "source": [
        "norm_conf_mat = conf_matrix / conf_matrix.sum(axis=1, keepdims=True)\n",
        "np.fill_diagonal(norm_conf_mat, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izfR95zeVubL"
      },
      "source": [
        "- Aquí vemos que, aunque nuestro algoritmo clasificó correctamente alrededor del 80% de nuestro dataset, siempre las clasificaciones erróneas fueron bastantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfEZuGBFm0c_"
      },
      "source": [
        "plt.matshow(norm_conf_mat, cmap=\"gray\")\n",
        "plt.grid(0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cme2d7xx9a3Y"
      },
      "source": [
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IKl_eJC90P_"
      },
      "source": [
        "precision_score(y_test, y_pred, average=\"weighted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqz6j3nkA8YN"
      },
      "source": [
        "recall_score(y_test, y_pred, average=\"weighted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMnbLJLvHm_k"
      },
      "source": [
        "f1_score(y_test, y_pred, average=\"weighted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaPFZms6B8Q3"
      },
      "source": [
        "cross_val_score(knn_clf, full_X_train, full_y_train, cv=4, scoring=\"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEjlZxauWJ4p"
      },
      "source": [
        "- En esta etapa de nuestro workflow, vamos a probar valores de *k* que mejoren nuestros resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waa3IMWhID4k"
      },
      "source": [
        "rec_scores = {}\n",
        "prec_scores = {}\n",
        "for k in tqdm(range(5, 16, 5)):\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    y_pred = knn_clf.predict(X_val)\n",
        "    rec_scores[k] = recall_score(y_val, y_pred, average=\"weighted\")\n",
        "    prec_scores[k] = precision_score(y_val, y_pred, average=\"weighted\")\n",
        "    print(f\"------Number of Neighbors: {k}--------\")\n",
        "    print(f\"Recall Score: {rec_scores[k]}\")\n",
        "    print(f\"Precision Score: {prec_scores[k]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBtQTPgKp0xe"
      },
      "source": [
        "total_data = {\n",
        "    \"neighbors\": list(range(5, 16, 5)),\n",
        "    \"rec_scores\": list(rec_scores.values()),\n",
        "    \"prec_scores\": list(prec_scores.values())\n",
        "}\n",
        "\n",
        "total_df = pd.DataFrame(total_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjk2U-ZSr-uO"
      },
      "source": [
        "plt.plot(total_df[\"neighbors\"], total_df[\"rec_scores\"])\n",
        "plt.xlabel(\"K Neighbors\")\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYasr6C0sFvc"
      },
      "source": [
        "plt.plot(total_df[\"neighbors\"], total_df[\"prec_scores\"])\n",
        "plt.xlabel(\"K Neighbors\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-FN0GmxseQx"
      },
      "source": [
        "knn_best = KNeighborsClassifier(n_jobs=-1)\n",
        "knn_best.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgrY4pzTssaD"
      },
      "source": [
        "random_sample = np.random.choice(np.arange(len(X_test)), 9)\n",
        "y_pred = knn_best.predict(X_test[random_sample])\n",
        "gen.visualize_subplot(\n",
        "    X_test[random_sample].reshape(-1, 28, 28),\n",
        "    y_pred, (3, 3), (6, 6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vsw3JHJWKsu"
      },
      "source": [
        "## **Trabajo Asíncrono**<a name=\"id9\"></a>\n",
        "\n",
        "- Con el mismo conjunto de imágenes, o con el de mnist de dígitos, hacer una partición del dataset que garatice que se mantienen las proporciones de cada clase por cada partición. Para esto, usarán [`StratifiedShufflesplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html). Para acceder al conjunto de dígitos de mnist, utilizar la siguiente línea de código: `mnist = datasets.fetch_openml(\"mnist_784\", version=1)`\n",
        "- Luego, construir una gráfica que muestre cómo varía el rendimiento sobre entrenamiento y validación a medida que aumenta el valor de k. A partir de esta gráfica, mostrar el rendimiento sobre el test set con el valor de k seleccionado.\n",
        "- A continuación, utilizar [`GridSearch`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) para determinar los mejores valores de los hiperparámetros. Para eso, averiguar sobre los siguientes hiperparámetros:\n",
        "    - `n_neighbors`\n",
        "    - `weights`\n",
        "    - `algorithm`\n",
        "- Por último, mostrar algunas imágenes del conjunto test con la clase estimada por el mejor clasificador obtenido en el punto anterior y mostrarlos en una cuadrícula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWgG37OTq3Fu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}